---
title: "Preparing Data for Regions"
output: 
  html_document:
    theme: yeti
---

Run in R 3.5.2

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(readxl))
suppressPackageStartupMessages(library(sf))
suppressPackageStartupMessages(library(shiny))
suppressPackageStartupMessages(library(mapview))
suppressPackageStartupMessages(library(leaflet))
suppressPackageStartupMessages(library(miniUI))
suppressPackageStartupMessages(library(DT))
suppressPackageStartupMessages(library(RColorBrewer))

source('snappingFunctions/snapFunctions.R') # snapping functions
source('snappingFunctions/snapOrganizationFunctions_citmon.R') # functions to do stuff with snapping functions
```

This document walks users through the requisite data preprocessing steps to prepare CitMon data for the 2020 IR Rivers and Streams Assessment and 2020 IR Lakes Assessment decision support applications. All initial steps are tested on 2016 and 2018 IR data and thus need to be rerun for 2020 data when those datasets become available.

This document demonstrates the necessary steps to get any regional office up to speed with prerequisite data organization/processing steps to run the 2020 IR. 

## Input data

#### CitMon
Bring in James' citizen monitoring dataset. I made it a csv before bringing it into RStudio to speed up rendering in app. 


```{r citmon2020, echo=F}
# too big to read in using read_excel
cit <- read_csv('C:/HardDriveBackup/R/GitHub/Rivers-StreamsAssessment/CitizenNonAgency/2020IR Citizen Ambient4.14.19 (2).csv') %>%
  dplyr::select(Group_Station_ID:`DEQ QA Comments`) # drop junk columns

#View(cit) # to look at dataset use this command
```

Bwork with spatial aspect, play with different ways of getting unique identifiers- group station Id (citizen assigned) and lat/long combos

```{r}

# Count occurrences of each Group_Station_ID
#cit1 <- dplyr::select(cit, Group_Station_ID, FDT_STA_ID, STA_DESC, Latitude, Longitude)

cit1 <- distinct(cit, Group_Station_ID, FDT_STA_ID, Latitude, Longitude, .keep_all =T)  %>%
  dplyr::select(Group_Station_ID,  FDT_STA_ID:FDT_SPG_CODE, Latitude:STA_CBP_NAME)# drop data to avoid any confusion

citCounts <- cit1 %>% 
  group_by(Group_Station_ID, Latitude, Longitude) %>%
  summarize(`Number of Station Sampling Events` = n())

citDetailscit <- cit1 %>% 
  group_by(Group_Station_ID, Latitude, Longitude) %>%
  mutate(`Number of Station Sampling Events` = n(),
         STA_DESC_norm = str_replace_all(STA_DESC, "[^[:alnum:]]", " ")) %>%
  ungroup() %>%
  distinct() %>%
  arrange(Group_Station_ID) 
citDetailscit$rowname <- c(1:nrow(citDetailscit))
# ugly addition singe add_rownames() alter data type


missingLocation <- filter(citDetailscit, is.na(Latitude) | is.na(Longitude) | Latitude == 0 | Longitude == 0)

citDetailscit_sf <- citDetailscit %>%
  filter(!(rowname %in% missingLocation$rowname)) %>%
  st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
           remove = F, # don't remove these lat/lon cols from df
           crs = 4326) # add coordinate reference system, needs to be geographic for now bc entering lat/lng,

```



Below is the table output. For this cycle, there are `r nrow(citDetailscit_sf)` unique stations sampled.

```{r conventionalsDistinctTable, echo=FALSE}
#View(citDetailscit_sf)
```


#### Statewide Assessment Layer

Because we cannot trust the assigned region code columns to give us the stations each region needs to assess, we must do a spatial subset of all unique stations in an IR window (citDetailscit_sf) by the Statewide Assessment Layer (filtered to our specified assessment region).

First, read in the statewide assessment layer.

```{r Statewide Assessment Layer, echo=FALSE, message=FALSE, warning=FALSE, results = 'hide'}
assessmentLayer <- st_read('GIS/AssessmentRegions_VA84_basins.shp') %>%
  st_transform( st_crs(4326)) # transform to WQS84 for spatial intersection
```

Now we are going to subset all unique stations from conventionals to a specific region. **This requires minimal user input**
 
Below are the regional codes you need to choose from 
```{r regionalCodes}
unique(assessmentLayer$ASSESS_REG)
```

Insert one of those unique codes in quotes into the chunk below. The commented out script serves as an example for you.

```{r establish regionCode}

regionCode <- ''
#regionCode <- 'BRRO'
```


Now we subset the layer based on your regionCode variable. We will use this variable again later in script.

```{r DEQ region selection}
assessmentLayerSelection <- filter(assessmentLayer, ASSESS_REG == regionCode)
#assessmentLayerSelection <- filter(assessmentLayer, ASSESS_REG ==    )
#assessmentLayerSelection <- filter(assessmentLayer, ASSESS_REG == 'BRRO')
```

Now we can subset unique citmon sites that fall into the assessment region we want.

```{r regional citmon, echo=F}

citmon_D_Region <- st_join(citDetailscit_sf, assessmentLayerSelection, join = st_intersects) %>%
  filter(!is.na(VAHU6)) %>% # only keep rows that joined
  dplyr::select(rowname, everything()) %>%
  select(-c(Shape_Leng, Shape_Area))
 
cat(paste('Below are the ',nrow(citmon_D_Region),' unique stations in the ', regionCode,' region.', sep=''))
```

```{r DistinctSitesInRegionTable, echo=FALSE}
#View(citmon_D_Region)
```


#### Last Cycle Estuary Assessment Layer

First thing we want to do now is filter out any sites that fall into the estuary AUs. These stations will have to be assessed through other means. **Note: you can download the table below for reference by running the commented script. These are all the stations within the selected assessment region that fall into an estuary AU based on the 2018 AU layers.**

The below chunk:
1) Reads in the 2018 draft estuary AU layer
2) Finds any sites that fall into an estuary AU polygon
  - see optional code to write those sites to a separate csv for future reference
3) Removes any estuarine sites from the data frame of unique conventionals sites that need AU and WQS information.

```{r Filter Out Estuary Statewide, echo=F, results = 'hide'}
estuaryAUs <- st_read('GIS/draft2018_spatialData/va_2018_aus_estuarine.shp') %>%
   st_transform( st_crs(4326)) # transform to WQS84 for spatial intersection


citmon_D_Region_Estuary <- st_join(citmon_D_Region, estuaryAUs, join = st_intersects) %>%
      filter(!is.na(OBJECTID))

# Remove the # if you want to run any of the lines below to save out the estuary sites to look at now. You will save them out at the end of the script, so this is optional right now.

#write.csv(conventionals_D_Region_Estuary, 'conventionals_D_Region_Estuary.csv', row.names = F) 
# you can change the file location like so:
#write.csv(conventionals_D_Region_Estuary, 'C:/Assessment/2020/dataProcessing/conventionals_D_Region_Estuary.csv', row.names = F) 

citmon_D_Region_NoEstuary <- filter(citmon_D_Region, !(FDT_STA_ID %in% citmon_D_Region_Estuary$FDT_STA_ID))
rm(estuaryAUs) # remove from memory
```

Below is the table of sites that are estuarine.

```{r estuarySitesTable, echo=F}
#View(citmon_D_Region_Estuary)
```

```{r inlineText2,echo=F}
cat(paste('Removing the above sites, the unique citmon sites have gone from ', nrow(citmon_D_Region),' to ',nrow(citmon_D_Region_NoEstuary),'. The next step is to find the lake/reservoir stations and link them to the appropriate AU and WQS for analyses in the Lake Assessment App.', sep=''))
```

### Assessment Unit Organization

#### Last Cycle Stations Table

The next step uses the 2018 IR station table as a starting point for linking the stations from the 2020 IR window to the appropriate AU. This is important to do prior to doing any spatial subsetting or snapping because most of the stations from the previous cycle already have QAed assessment unit information readily available and organized. It is computationally less expensive to save any necessary spatial subsetting or snapping to lake/reservoir polygons or stream segments until all joins that can be done with table methods are first completed.

```{r previousCycleStationTable}
# Statewide (mostly final) stations table from Cleo ( April 19, 2019), I exported the table to excel to avoid changing to 32bit version of R. DEQ only has 32 bit version of MS Office products, so connecting to Access databases requires changing the R program slightly, which is a pain in the middle of the script and going from 64 bit to 32 bit version of R just to connect to Access unnecessarily slows other processing times, so I made it easier with the Excel export step.

previousCycleStationTable <- read_excel('data/final2020data/tbl_ir_mon_stations.xlsx') %>%
  filter(REGION == regionCode)


# Join unique sites from current cycle to last cycle's station table to see what already has AU info
citmon_D_Region_AU <- mutate(citmon_D_Region_NoEstuary, STATION_ID = FDT_STA_ID) %>% # make joining column
    left_join(previousCycleStationTable, by='STATION_ID') # join to get ID305B info

# Only work with sites that don't already have AU info
citmon_D_Region_NoAU <- filter(citmon_D_Region_AU, is.na(ID305B_1))
#View(conventionals_D_Region_NoAU)  

# Keep the stations with AU information to the side for later use
citmon_D_Region_AU <- filter(citmon_D_Region_AU, !is.na(ID305B_1))
#View(conventionals_D_Region_AU)

# make sure we didnt lose anyone
nrow(citmon_D_Region_NoAU) + nrow(citmon_D_Region_AU) == nrow(citmon_D_Region) 

rm(citmon_D_Region_NoEstuary) # clean up workspace
```


The table below illustrates all the  unique citmon sites that matched stations from 2018 IR station table by STATION_ID/FDT_STA_ID.
```{r citmon_D_Region_AU, echo=F}
#View(citmon_D_Region_Estuary)
```

The table below illustrates all the unique citmon sites that did not match any stations from 2018 IR station table by STATION_ID/FDT_STA_ID.
```{r citmon_D_Region_NoAU, echo=F}
#View(citmon_D_Region_Estuary)
```

#### Last Cycle Reservoir Assessment Layer 

```{r inlineText3, echo=F}
cat(paste('Working with the ', nrow(citmon_D_Region_NoAU),' sites that did not connect to a STATION_ID/FDT_STA_ID from last cycle, we will first do a spatial join to the reservoir/lakes AUs to see if any sites are lake sites before using the more computationally intensive stream snapping process.', sep=''))
```

```{r reservoirStatewide, echo=F, results = 'hide'}
reservoirAUs <- st_read('GIS/draft2018_spatialData/va_2018_aus_reservoir.shp') %>%
   st_transform( st_crs(4326)) # transform to WQS84 for spatial intersection

# find lake AUs, if Any
citmon_D_Region_Lakes <- st_join(citmon_D_Region_NoAU, reservoirAUs, join = st_intersects) %>%
  filter(!is.na(OBJECTID)) %>% # only keep rows where join successful
  mutate(ID305B_1 = ID305B) %>% # rename ID305B_1 field by the joined reservoir ID305B
  dplyr::select(names(citmon_D_Region_AU))

# add lake AU sites to citmon_D_Region_AU
citmon_D_Region_AU <- rbind(citmon_D_Region_AU, citmon_D_Region_Lakes)

# and remove lake AU sites from the sites without AUs
citmon_D_Region_NoAU <- filter(citmon_D_Region_NoAU, !(rowname %in% citmon_D_Region_Lakes$rowname))#filter(citmon_D_Region_NoAU, !(FDT_STA_ID %in% citmon_D_Region_Lakes$FDT_STA_ID))

# make sure we didn't lose anyone
nrow(citmon_D_Region_NoAU) + nrow(citmon_D_Region_AU) == nrow(citmon_D_Region) 

rm(reservoirAUs); rm(citmon_D_Region_Lakes) # remove from memory
```

#### Last Cycle Riverine Assessment Layer

```{r inlineText4, echo=F}
cat(paste('The last step for AU organization after we have removed any estuary sites and dealt with all sites that are in lake AUs is to automatically snap the', nrow(citmon_D_Region_NoAU), 'remaining sites to riverine AUs.',sep=' '))
```

A custom, automated snapping tool is run below. The function buffers each site using the user defined sequence (bufferDistances). You do not need to change the bufferDistances variable, but commented out code demonstrates how you can if you want to increase/decrease the maximum buffer distance or breaks. 

The function first buffers the site the lowest input buffer distance, then it intersects that buffer with the input assessment unit layer. If no segments are retrieved in the given buffer distance, then the next highest buffer is tried and so on until the maximum buffer distance is tested. If one or more segments are captured in a given buffer, then the buffering process stops, returns the snapped stream segment information, and moves to the next site. If more that one site is captured in a single buffer, then those segments will be provided to the user to choose from in an interactive gadget to keep only one AU per site after all buffering has finished. If the maximum buffer distance does not yield any stream segments, then those sites will be returned to the user for special QA and manual table completion after the report is finished. It is advised that you investigate the site further and potentially correct site coordinates in CEDS to prevent future problems. 

**Note: All stations you want to use in the assessment applications require AU information. If no AU information is provided to the app with a StationID, then the station will not appear in the app and could potentially be lost until further QA.**

The next chunk will bring in the riverine AU layer from the previous cycle, complete the snapping tool, ask for user input in the form of a gadget in the viewer panel if sites snap to multiple segments in a single buffer, and then organize results. You can see the progress and results of the tool below the chunk.

```{r riverineAUsnap, echo=FALSE}
# use a custom snapping tool to snap and organize 
riverineAUs <-  st_read('GIS/draft2018_spatialData/va_2018_aus_riverine.shp') %>%
  st_transform(crs = 102003)  # transform to Albers for spatial intersection

Regional_Sites_AU <- snapAndOrganizeAU_citmon(citmon_D_Region_NoAU, riverineAUs,  bufferDistances = seq(10,80,by=10))
# this buffers up to 100 m by 10 m
#Regional_Sites_AU <- snapAndOrganizeAU(citmon_D_Region_NoAU, riverineAUs, bufferDistances = seq(10,100,by=10)) 
# this buffers up to 50 m by 5 m
#Regional_Sites_AU <- snapAndOrganizeAU(citmon_D_Region_NoAU, riverineAUs, bufferDistances = seq(10,50,by=5))

rm(riverineAUs) #clean up workspace

```

We can now filter the results to figure out sites that worked and sites that still need special attention. 

Below are the sites that still need work.
```{r conventionals_D_Region_NoAU still, echo=F}
citmon_D_Region_NoAU <- filter(Regional_Sites_AU, is.na(ID305B_1)) %>%
  left_join(dplyr::select(citmon_D_Region, rowname:FDT_STA_ID, Latitude, Longitude) %>% st_drop_geometry(), 
            by = c( 'FDT_STA_ID', 'Latitude', 'Longitude')) %>% # join back to original dataset that went into snapping function to get rowname and Group_Station_ID
  dplyr::select(rowname, Group_Station_ID, everything())

#View(conventionals_D_Region_NoAU)
```

These sites will still be snapped to WQS, but they still need AU information before they can be run through the assessment applications. A reminder at the end of the report will encourage users to complete that step.


Below are the sites that worked. These can be combined with our conventionals_D_Region_AU layer now. 
```{r Regional_Sites_AUgood, echo=F}
Regional_Sites_AU <- filter(Regional_Sites_AU, !is.na(ID305B_1)) %>%
  left_join(dplyr::select(citmon_D_Region, rowname:FDT_STA_ID, Latitude, Longitude) %>% st_drop_geometry(), 
            by = c( 'FDT_STA_ID', 'Latitude', 'Longitude')) %>% # join back to original dataset that went into snapping function to get rowname and Group_Station_ID
  dplyr::select(rowname, Group_Station_ID, everything())

#View(Regional_Sites_AU)

# only bind citmon_D_Region_NoAU if it has records, otherwise the bind won't work
if(nrow(Regional_Sites_AU) > 0 & nrow(citmon_D_Region_NoAU) > 0 ){
  citmon_D_Region_AU <- rbind(citmon_D_Region_AU, 
                              Regional_Sites_AU %>% st_transform( st_crs(4326)), 
                              citmon_D_Region_NoAU %>% st_transform( st_crs(4326)))}
if(nrow(Regional_Sites_AU) > 0 & nrow(citmon_D_Region_NoAU) == 0 ){
  citmon_D_Region_AU <- rbind(citmon_D_Region_AU, 
                              Regional_Sites_AU %>% st_transform( st_crs(4326)))}
if(nrow(Regional_Sites_AU) == 0 & nrow(citmon_D_Region_NoAU) > 0 ){
  citmon_D_Region_AU <- rbind(citmon_D_Region_AU, 
                               citmon_D_Region_NoAU %>% st_transform( st_crs(4326)))}

# save just AU info
write.csv(citmon_D_Region_AU, 'processedStationData/citmon/BRRO_AU_citmon.csv', row.names = F)
st_write(citmon_D_Region_AU, 'processedStationData/citmon/BRRO_AU_citmon.shp')

rm(Regional_Sites_AU) # clean up workspace
```




































### Assessment QA time out

So those above datasets need to be thoroughly reviewed and QAed to make sure each station is in fact unique and have all the correct associated data with it. When that is done, you can bring the data back in.

```{r citmon timeIn}
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(readxl))
suppressPackageStartupMessages(library(sf))
suppressPackageStartupMessages(library(shiny))
suppressPackageStartupMessages(library(mapview))
suppressPackageStartupMessages(library(leaflet))
suppressPackageStartupMessages(library(miniUI))
suppressPackageStartupMessages(library(DT))
suppressPackageStartupMessages(library(RColorBrewer))

source('snappingFunctions/snapFunctions.R') # snapping functions
source('snappingFunctions/snapOrganizationFunctions_citmon.R') # functions to do stuff with snapping functions

citMonAU <- read_excel('processedStationData/citmon/CitMon_stations_QA_Final.xlsx','Paula_NEW')

monitor <- read_excel('processedStationData/citmon/MONITOR17May2019.xlsx')
```
 
First let's delete any weird things they want gone and then it is time to correct the FDT_STA_ID based on Mary and Paula's calls. We will replace the FDT_STA_ID assigned by James with Mary and Paula's call and then update all the associated information accordingly based on the MONITOR database.

```{r delete weird things}
# get the rows they want deleted
citMonAU1 <- filter(citMonAU, !(grepl('delete',citMonAU$`AllMon station`)))
```

Now override James' assigned station id and join to MONITOR to get Mary/Paula approved coordinates. Not going to fix AU's yet because known issues with last cycle ID305B_1 column.

```{r Mary/paula overrule}
citMonAU2 <- mutate(citMonAU1, FDT_STA_ID = ifelse(is.na(`AllMon station`), FDT_STA_ID, `AllMon station`),
                    STATION = FDT_STA_ID) 
citMonAU3 <- left_join(citMonAU2, monitor, by='STATION') %>%
  dplyr::select(-c(`AllMon station`))



notfine <- filter(citMonAU3, is.na(TRECORD))
nrow(notfine) == 0 #@ yayyyy no issues!

rm(notfine); rm(citMonAU2)


citMonAU4 <- mutate(citMonAU3, FDT_STA_ID = STATION, STA_DESC = DESCRIP, Latitude = MDEC_LAT, Longitude = MDECLONG) %>% # keep only information from MONITOR, overwrite James
  dplyr::select(`AU to connect to`,everything()) %>%
  select(-c(Group_Station_ID,STATION_ID, ID305B_2:Larry))
rm(citMonAU3)
```


Now join back to station table from last cycle to get better AU information since mary/paula changed some AUs.

```{r fix more station info}
regionCode <- 'BRRO'

previousCycleStationTable <- read_excel('data/final2020data/tbl_ir_mon_stations.xlsx') %>%
  filter(REGION == regionCode)

citMonAU5 <- mutate(citMonAU4, ID305B_1 = ifelse(is.na(`AU to connect to`), ID305B_1, `AU to connect to`), # take Mary/Paula desired AUs over whatever they called it last cycle
                    STATION_ID = FDT_STA_ID) %>% # make joining column
  left_join(previousCycleStationTable, by= c('STATION_ID')) %>%
  select(`AU to connect to`, ID305B_1.x, ID305B_1.y, everything()) %>%
  mutate(sameAU = ifelse(ID305B_1.x==ID305B_1.y, T, F)) %>%
  select(sameAU, everything())

# so not all new AUs are same as what they called thigns last time. Best to take their current calls

citMonAU6 <- rename(citMonAU5, ID305B_1 = "ID305B_1.x") %>% # keep mary and paula call
  select(rowname:STATION_ID,ID305B_1, everything()) %>%
  select(-c(sameAU, `AU to connect to`,ID305B_1.y))


cit <- citMonAU6 %>%
  st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
           remove = F, # don't remove these lat/lon cols from df
           crs = 4326) # add coordinate reference system, needs to be geographic for now bc entering lat/lng,


rm(monitor);rm(previousCycleStationTable); rm(citMonAU); rm(citMonAU4);rm(citMonAU5);rm(citMonAU6)
```


So now we have a dataset that has corrected station ID's and AU's. Note that there are still duplicated entries, but that is good until I limit out the raw data by the rowname field.


Now time to do WQS things.




### WQS Organization

Now all the stations need water quality standards attached as the final data organization step in order to be used by the assessment apps. Regardless of whether or not a site snapped to an AU, the report will walk users through connecting each site to WQS features. First, we will start with lakes to save the computationally heavier stream snapping tool for only the sites that need it.

#### Updated Reservoir WQS layer

We will bring in the updated WQS reservoir layer, to attach to stations that are in lacustrine AU's. There are three methods below for identifying your lacustrine stations to choose from. Only one needs to be run to establish a reservoirStations object to join to reservoir WQS information. 

```{r find Lacustrine AUs, echo=F}
# Bring reservoir AUs back in just for ID305B options
#reservoirAUs <- as.character(st_read('GIS/draft2018_spatialData/va_2018_aus_reservoir.shp')$ID305B)

# Method 1: use lake AU's
# Separate lake stations from stream stations
# this is one way, but since we have Paula's official station table, we will use that instead
#reservoirStations <- filter(conventionals_D_Region_AU, ID305B_1 %in% unique(reservoirAUs) |
#                              ID305B_2 %in% unique(reservoirAUs) | ID305B_3 %in% unique(reservoirAUs))

# Method 2: Look for 'L' designation in station type codes
#reservoirStations <- filter(conventionals_D_Region_AU,  grepl("L",STATION_TYPE_1) | grepl("L",STATION_TYPE_2) |
#                               grepl("L",STATION_TYPE_3))
# the L searching method is safer because it captures the ,L situations where L tacked on to other station designation

# Method 3: use original lake stations table
# but the safest method for BRRO is to just use Paula's stations table
#reservoirStations <- filter(conventionals_D_Region_AU, FDT_STA_ID %in% stationTableLake$STATION_ID)



# Use Assessor defined AU's to figure out if lake or stream

cit$RorL <- NA
for(i in 1: nrow(cit)){
  cit$RorL[i] <- substr(strsplit(as.character(cit$ID305B_1[i]), '-')[[1]][2] ,4,4)
  
}


# split into riverine or lake
reservoirStations <- filter(cit, RorL == 'L')

# Bring in reservoir WQS
reservoirWQS <- st_read('GIS/updatedWQS/reservoir.shp') %>%
   st_transform( st_crs(4326)) # transform to WQS84 for spatial intersection

# find lake WQS
reservoirStationsWQS <- st_join(reservoirStations, reservoirWQS, join = st_intersects) %>%
  filter(!is.na(OBJECTID)) # only keep rows where join successful
#View(reservoirStationsWQS)

# add back in duplicate issues where lacustrine and riverine, important where stations used for both riverine and lacustrine assessments
notInReservoirWQS <- unique(reservoirStations$FDT_STA_ID)[!( unique(reservoirStations$FDT_STA_ID) %in% unique(reservoirStationsWQS$FDT_STA_ID))]



# So these need to be fixed by hand
problem <- filter(reservoirStations, FDT_STA_ID %in% notInReservoirWQS)
st_write(problem,'processedStationData/citmon/lakeStationsNotInLakeWQS2.shp')

# fix individual problem stations
reservoirStationsWQS_fixed <- mutate(problem, OBJECTID = ifelse(FDT_STA_ID == '4ALVL-CR26-FC', 21455, NA)) %>% # WQS layer error?
   mutate(OBJECTID = ifelse(FDT_STA_ID == '4AROA-R29-FC', 21455, OBJECTID)) %>% # just barely outside WQS layer
  st_set_geometry(NULL) %>% # get rid of spatial aspect to force a left join using spatial objects
  left_join(reservoirWQS %>% st_set_geometry(NULL), by='OBJECTID') %>%
  # turn back to spatial layer to smash together with good stations
  st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
           remove = F, # don't remove these lat/lon cols from df
           crs = 4326) %>%# add coordinate reference system, needs to be geographic for now bc entering lat/lng,
  bind_rows(reservoirStationsWQS)

## make sure we didnt lose anyone
nrow(reservoirStations) == nrow(reservoirStationsWQS) + length(notInReservoirWQS)


# Run all stations that didn't connect to lake WQS polygon through streams snapping tool
riverineStationsWQS <- filter(cit, !(FDT_STA_ID %in% reservoirStationsWQS_fixed$FDT_STA_ID))
                              
# Make sure we didn't lose anyone
nrow(riverineStationsWQS) + nrow(reservoirStationsWQS_fixed) == nrow(cit) 

rm(reservoirWQS); rm(reservoirStations); rm(problem); rm(reservoirStationsWQS) # clean up workspace
```


All stations that did not attach to a lake WQS will be run below as a stream site. This stream WQS snapping tool behaves the same way as the AU snapping tool; however, we are bringing in basins for snapping one at a time because the WQS files are so large. The tool will handle the organization and snapping by basin. You are only responsible for using the gadget that pops up in the Viewer panel when each basin prompts you. You can see the progress below the chunk. This is the most computationally heavy part of the process.


```{r attachStreamWQS, echo=F}
regionalOutput <- list()
# Loop through multiple basins
for (i in 1:length(unique(riverineStationsWQS$Basin))){
  Regional_Sites_AU_basin <- filter(riverineStationsWQS, Basin %in% unique(riverineStationsWQS$Basin)[i] )
  WQSsnaps <- snapAndOrganizeWQS_citmon(Regional_Sites_AU_basin, 'GIS/updatedWQS/', 
                                 basinNameSwitcher(unique(Regional_Sites_AU_basin$Basin)),  seq(10,80,by=10))
  regionalOutput[[i]] <- WQSsnaps
}
```

Now the list gets smashed together into a single dataframe (tibble).

```{r smash}
# smash it all back together, change from lists to single dataframe
RegionalResults <- do.call("rbind", lapply(regionalOutput, data.frame))
#View(RegionalResults)
rm(WQSsnaps)# clean up workspace
```

Below is the table of riverine stations without WQS information attached. These will be returned to the user with a full list of stations that need individual attention. 

```{r RegionalResults table, echo=F}
RegionalResultsNoWQS <- filter(RegionalResults, is.na(OBJECTID))
#View(RegionalResultsNoWQS)
```

Lastly, we need to combine the lake and stream stations into a single dataframe. After we combine we will separate out the stations that need either AU or WQS manual entry. We suggest using GIS to find this information. 

```{r combine WQS, echo=F}
# Reorder columns in reservoirStationsWQS to match RegionalResults to allow row binding
reservoirStationsWQS <- mutate(reservoirStationsWQS_fixed, Point.Unique.Identifier=NA, 
                               Buffer.Distance=NA, StreamType=NA )

reservoirStationsWQS1 <- reservoirStationsWQS %>%  as.data.frame() %>%
  dplyr::select(rowname, FDT_STA_ID:COMMENTS, Point.Unique.Identifier, Buffer.Distance, 
         OBJECTID:FCode, BASIN, WQS_COMMEN, WATER_NAME, SEC, CLASS, 
         SPSTDS, SECTION_DE, Basin_Code, PWS, Trout, Edit_Date, StreamType, 
         Tier_III, Backlog, Shape_Leng) %>%
  select(-`Number of Station Sampling Events`)

almostReady <- select(RegionalResults,-c( Number.of.Station.Sampling.Events, RorL  ))

RegionalResultsCombined <- rbind(almostReady, reservoirStationsWQS1)
#View(RegionalResultsCombined)
rm(almostReady);rm(Regional_Sites_AU_basin);rm(regionalOutput);rm(RegionalResultsNoWQS);rm(reservoirStationsWQS_fixed);rm(riverineStationsWQS);rm(reservoirStationsWQS);rm(RegionalResults)
```




Now time to get raw data from citmon sites into CEDS format. Important to do this before bringing in non Agency data because of potential for mixing up citmon and non agency rownames when looking at raw data.



```{r citMon Raw organization}
citRaw <- read_csv('C:/HardDriveBackup/R/GitHub/Rivers-StreamsAssessment/CitizenNonAgency/2020IR Citizen Ambient4.14.19 (2).csv') %>%
  dplyr::select(Group_Station_ID:`DEQ QA Comments`) # drop junk columns

cit1 <- distinct(citRaw, Group_Station_ID, FDT_STA_ID, Latitude, Longitude, .keep_all =T)  %>%
  dplyr::select(Group_Station_ID,  FDT_STA_ID:FDT_SPG_CODE, Latitude:STA_CBP_NAME)# drop data to avoid any confusion

citCounts <- cit1 %>% 
  group_by(Group_Station_ID, Latitude, Longitude) %>%
  summarize(`Number of Station Sampling Events` = n())

citDetailscit <- cit1 %>% 
  group_by(Group_Station_ID, Latitude, Longitude) %>%
  mutate(`Number of Station Sampling Events` = n(),
         STA_DESC_norm = str_replace_all(STA_DESC, "[^[:alnum:]]", " ")) %>%
  ungroup() %>%
  distinct() %>%
  arrange(Group_Station_ID) 
citDetailscit$rowname <- c(1:nrow(citDetailscit))
# ugly addition singe add_rownames() alter data type


# now attach the rowname identifier to raw data
citRaw1 <- left_join(citRaw, 
                     dplyr::select(citDetailscit, Group_Station_ID, FDT_STA_ID,
                                   STA_DESC, Latitude, Longitude, rowname),
                     by= c("Group_Station_ID", "FDT_STA_ID", "STA_DESC","Latitude" ,                        
                           "Longitude"  ))
# make sure citRaw1 has same rows as citRaw or some weird join may have happened
nrow(citRaw1)==nrow(citRaw)

# now just get rows with rowname ( unique stations)
citRaw2 <- filter(citRaw1, !is.na(rowname)) %>%
  select(rowname, everything())




# Now filter raw cit data to just sites Mary wants to assess
citRaw3 <- filter(citRaw2, rowname %in% unique(RegionalResultsCombined$rowname)[!is.na(unique(RegionalResultsCombined$rowname))])


# Now overwrite the names James gave raw data to match Mary/Paula call so i can find them in the app 
citRawFinal <- left_join(citRaw3, 
                     select(RegionalResultsCombined, rowname, FDT_STA_ID, STA_DESC, Latitude, Longitude), by='rowname') %>%
  mutate(FDT_STA_ID = FDT_STA_ID.y, STA_DESC = STA_DESC.y, Latitude = Latitude.y, Longitude = Longitude.y) %>% # keep what mary/paula called
  select(-c(FDT_STA_ID.x, STA_DESC.x, Latitude.x, Longitude.x, FDT_STA_ID.y, STA_DESC.y, Latitude.y, Longitude.y)) %>%
  select(names(citRaw3)) # reorder columns to original format



# now make citRawFinal match conventionals
conventionals <- read_csv('data/final2020data/CEDSWQM_2020_IR_DATA-CONVENTIONALS_20190305.csv') %>%
  filter(!is.na(Latitude)|!is.na(Longitude)) # remove sites without coordinates
conventionals$FDT_DATE_TIME2 <- as.POSIXct(conventionals$FDT_DATE_TIME, format="%m/%d/%Y %H:%M") # turn character date format into proper date time format


# first make FDT_DATE_TIME column the same
citRawFinal$FDT_DATE_TIME <- as.POSIXct(citRawFinal$FDT_DATE_TIME, format="%m/%d/%y %H:%M:%S")
conventionals$FDT_DATE_TIME <- as.POSIXct(conventionals$FDT_DATE_TIME, format="%m/%d/%Y %H:%M")

citRawFinal1 <- mutate(citRawFinal, 
                      FDT_COMMENT = paste('FDT_COMMENT:', FDT_COMMENT, 
                                          'QAQC_Comments:',QAQC_Comments,
                                          'DEQ QA Comments:',`DEQ QA Comments`),
                      FDT_DATE_TIME2 = as.POSIXct(FDT_DATE_TIME, format="%m/%d/%y %H:%M")) %>%
  dplyr::select(names(conventionals))


# save summary of parameters taken for stations table, things that don't fit into conventionals
citRawFinalForComments <- mutate(citRawFinal, 
                      FDT_COMMENT = paste('FDT_COMMENT:', FDT_COMMENT, 
                                          'QAQC_Comments:',QAQC_Comments,
                                          'DEQ QA Comments:',`DEQ QA Comments`),
                      FDT_DATE_TIME2 = FDT_DATE_TIME)
write.csv(citRawFinalForComments, 'data/CitMonOrganizedDataBRRO.csv', row.names = F)

# Combine with real conventionals
conventionals2 <- rbind(conventionals, citRawFinal1)

rm(citRaw); rm(citRaw1); rm(citRaw2);rm(citRaw3); rm(citRawFinal1); rm(citDetailscit); rm(cit1); rm(citMonAU1)
```



Now bring in nonagency and start by fixing raw data before smashing in non agency stations into station table 2.0.

```{r nonagency}

templateR <- read_csv('processedStationData/final2020data/RegionalResultsRiverine_BRROFINAL.csv') 
templateL <- read_csv('processedStationData/final2020data/RegionalResultsLake_BRROFINAL.csv')
templateL$STA_CBP_NAME <- as.character(templateL$STA_CBP_NAME)
templateL$GNIS_ID <- as.character(templateL$GNIS_ID)



template <- bind_rows(templateR, templateL) %>%
  mutate(rowname = NA) %>% # keep column for now
  select(rowname, everything())
rm(templateL);rm(templateR)


## Bring in non Agency stations

nonA <- read_csv('processedStationData/citmon/RegionalResultsCombined_NonAgencyBRRO.csv') %>%
  mutate(VAHU6=VAHU6.x, Huc6_Vahu6 = VAHU6.x) %>%
  dplyr::select( names(template))


nonARaw <- read_csv('C:/HardDriveBackup/R/GitHub/Rivers-StreamsAssessment/CitizenNonAgency/2020IR NONA ambient.csv')  %>%
  dplyr::select(Group_Station_ID:`DEQ QA Comments`) # drop junk columns for now


nonA1 <- distinct(nonARaw, Group_Station_ID, FDT_STA_ID, Latitude, Longitude, .keep_all =T)  %>%
  dplyr::select(Group_Station_ID,  FDT_STA_ID:FDT_SPG_CODE, Latitude:STA_CBP_NAME)# drop data to avoid any confusion

nonACounts <- nonA1 %>% 
  group_by(Group_Station_ID, Latitude, Longitude) %>%
  summarize(`Number of Station Sampling Events` = n())

nonADetailscit <- nonA1 %>% 
  group_by(Group_Station_ID, Latitude, Longitude) %>%
  mutate(`Number of Station Sampling Events` = n(),
         STA_DESC_norm = str_replace_all(STA_DESC, "[^[:alnum:]]", " ")) %>%
  ungroup() %>%
  distinct() %>%
  arrange(Group_Station_ID) 
nonADetailscit$rowname <- c(1:nrow(nonADetailscit))
# ugly addition singe add_rownames() alter data type

# now attach the rowname identifier to raw data
nonARaw1 <- left_join(nonARaw, 
                     dplyr::select(nonADetailscit, Group_Station_ID, FDT_STA_ID,
                                   STA_DESC, Latitude, Longitude, rowname),
                     by= c("Group_Station_ID", "FDT_STA_ID", "STA_DESC","Latitude" ,                        
                           "Longitude"  ))
# make sure citRaw1 has same rows as citRaw or some weird join may have happened
nrow(nonARaw1)==nrow(nonARaw)

# now just get rows with rowname ( unique stations)
nonARaw2 <- filter(nonARaw1, !is.na(rowname))




# Now filter raw cit data to just sites Mary wants to assess
nonARaw3 <- filter(nonARaw2, rowname %in% unique(nonA$rowname)[!is.na(unique(nonA$rowname))])


# Now overwrite the names James gave raw data to match Mary/Paula call so i can find them in the app 
nonARawFinal <- left_join(nonARaw3, 
                     select(nonA, rowname, FDT_STA_ID, STA_DESC, Latitude, Longitude, VAHU6), by='rowname') %>%
  mutate(FDT_STA_ID = FDT_STA_ID.y, STA_DESC = STA_DESC.y, Latitude = Latitude.y, Longitude = Longitude.y, Huc6_Vahu6 = VAHU6) %>% # keep what mary/paula called
  select(-c(FDT_STA_ID.x, STA_DESC.x, Latitude.x, Longitude.x, FDT_STA_ID.y, STA_DESC.y, Latitude.y, Longitude.y)) %>%
  select(names(nonARaw3)) # reorder columns to original format


# first make FDT_DATE_TIME column the same
nonARawFinal$FDT_DATE_TIME <- as.POSIXct(nonARawFinal$FDT_DATE_TIME, format="%m/%d/%y %H:%M")



# now make citRawFinal match conventionals
#conventionals <- read_csv('data/final2020data/CEDSWQM_2020_IR_DATA-CONVENTIONALS_20190305.csv') %>%
#  filter(!is.na(Latitude)|!is.na(Longitude)) # remove sites without coordinates
#conventionals$FDT_DATE_TIME2 <- as.POSIXct(conventionals$FDT_DATE_TIME, format="%m/%d/%Y %H:%M") # turn character date format into proper date time format

nonARawFinal1 <- mutate(nonARawFinal, 
                      FDT_COMMENT = paste('FDT_COMMENT:', FDT_COMMENT, 
                                          'QAQC_Comments:',QAQC_Comments,
                                          'DEQ QA Comments:',`DEQ QA Comments`),
                      FDT_DATE_TIME2 = FDT_DATE_TIME) %>%
                      #FDT_DATE_TIME2 = as.POSIXct(FDT_DATE_TIME, format="%m/%d/%y %H:%M")) %>%
  dplyr::select(names(conventionals))

# save summary of parameters taken for stations table, things that don't fit into conventionals
nonARawFinalForComments <- mutate(nonARawFinal, 
                      FDT_COMMENT = paste('FDT_COMMENT:', FDT_COMMENT, 
                                          'QAQC_Comments:',QAQC_Comments,
                                          'DEQ QA Comments:',`DEQ QA Comments`),
                      FDT_DATE_TIME2 = FDT_DATE_TIME)
write.csv(nonARawFinalForComments, 'data/NonAgencyOrganizedDataBRRO.csv', row.names = F)


# Combine with real conventionals
conventionals3 <- bind_rows(conventionals2, nonARawFinal1)


# keep only 1 record from each station, datetime combination since James included same data in two original datasets
#conventionals4 <- distinct(conventionals3, FDT_STA_ID, FDT_DATE_TIME2, FDT_DEPTH, .keep_all = T)


#nrow(conventionals3)-nrow(conventionals4) # so lost 454 duplicated rows 

rm(nonARaw); rm(nonARaw1); rm(nonARaw2);rm(nonARaw3); rm(nonARawFinal1); rm(nonADetailscit); rm(nonA1); rm(nonACounts); rm(citCounts)





# save new Conventionals data
write.csv(conventionals3, 'data/conventionals_final2020_citmonNonAgency.csv', row.names = F)

```


**UPDATE 6/19/2019** James identified that TP and ECOLI can be moved to appropriate columns. Necessary for lake app more than streams because of the volume of messed up data.

```{r TP ECOLI conventionals fix}
conventionals3 <- read_csv('data/conventionals_final2020_citmonNonAgency.csv')

conventionals4 <- mutate(conventionals3, 
                         `PHOSPHORUS_mg/L` = case_when(STA_LV3_CODE %in% c("CMON","NONA") ~ `PHOSPHORUS_TOTAL_00665_mg/L`,
                                                       TRUE ~ `PHOSPHORUS_mg/L`), # alternative keep original
                         `RMK_PHOSPHORUS` = case_when(STA_LV3_CODE %in% c("CMON","NONA") ~ `RMK_00665`,
                                                      TRUE ~ `RMK_PHOSPHORUS`), # alternative keep original
                         `ECOLI_CFU/100mL` = case_when(STA_LV3_CODE %in% c("CMON","NONA") ~ `E._COLI_31648_NO/100mL`,
                                                       TRUE ~ `ECOLI_CFU/100mL`), # alternative keep original
                         `ECOLI_RMK` = case_when(STA_LV3_CODE %in% c("CMON","NONA") ~ `RMK_31648`,
                                                 TRUE ~ `ECOLI_RMK`), # alternative keep original
                         ) %>%
    # and now take those data points away so they arent double counted in pooling efforts
  mutate(`PHOSPHORUS_TOTAL_00665_mg/L` = ifelse(STA_LV3_CODE %in% c("CMON","NONA"), NA,`PHOSPHORUS_TOTAL_00665_mg/L`),
         `RMK_00665` = ifelse(STA_LV3_CODE %in% c("CMON","NONA"), NA,`RMK_00665`),
         `E._COLI_31648_NO/100mL` = ifelse(STA_LV3_CODE %in% c("CMON","NONA"), NA, `E._COLI_31648_NO/100mL`),
         `RMK_31648` = ifelse(STA_LV3_CODE %in% c("CMON","NONA"), NA,`RMK_31648`))
  
write.csv(conventionals4, 'data/conventionals_final2020_citmonNonAgency_TP_ECOLIupdates.csv', row.names = F)
#saveRDS(conventionals4, 'data/conventionals_final2020_citmonNonAgency_TP_ECOLIupdates.RDS')

```




Now time to combine James' citmon stations that Mary and Paula wanted collapsed since we have used their rowname field to get raw data from conventionals.

```{r collapseStations}
# number of stations that need to be dealt with
nrow(dupes <- RegionalResultsCombined %>% group_by(FDT_STA_ID) %>% mutate(nrows=n()) %>% dplyr::select(nrows,everything()) %>%filter(nrows>1))/2

unique(dupes$FDT_STA_ID)

# Go through each pair and make sure no major differences
View(filter(dupes, FDT_STA_ID == '4AROA-B08-FC')) # cool, choose 410, not 411
View(filter(dupes, FDT_STA_ID == '4AROA-R07-FC')) # cool 1073 bc not betty's creek, not 1055
View(filter(dupes, FDT_STA_ID == '4AROA-R09-FC'))# cool 1074 bc not betty's creek, not 1056
View(filter(dupes, FDT_STA_ID == 'LVLAROA140.66'))# cool 35, not 36
View(filter(dupes, FDT_STA_ID == 'LVLAROA157.95'))# cool 55, not 56

RegionalResultsCombined1 <- filter(RegionalResultsCombined, !(rowname %in% c(411, 1055, 1056, 36, 56)))

rm(dupes)

```







A few more data processing steps to smash citmon into stations table 2.0 for app.

```{r last bit}
#template <- read_csv('processedStationData/final2020data/RegionalResultsRiverine_BRROFINAL.csv') %>% 
#  mutate(rowname = NA, VAHU6 = Huc6_Vahu6) %>% # keep column for now
#  select(rowname, everything())

RegionalResultsCombined2 <- mutate(RegionalResultsCombined1, VAHU6=VAHU6.x) %>%
  dplyr::select( names(template))

# ADD TO final stations table 
RegionalResults3 <- template %>%
  rbind(RegionalResultsCombined2)

```



Now combine unique non agency stations to stations table for app.

```{r nonagency to stationstable}
# Smash together with non agency
RegionalResults4 <-  rbind(RegionalResults3, nonA)


rm(nonA); rm(template); rm(citMonAU1);rm(RegionalResults3);rm(RegionalResultsCombined);rm(RegionalResultsCombined2);rm(cit)



# Last minute fixes of any lingering VAC's, change to VAW
View(dplyr::filter(RegionalResults4, grepl("VAC",ID305B_1)))
RegionalResults5 <- mutate(RegionalResults4, ID305B_1 = ifelse(grepl("VAC",ID305B_1) == T, gsub("VAC",'VAW',ID305B_1),ID305B_1 ))
View(dplyr::filter(RegionalResults5, grepl("VAC",ID305B_1)))

# Last minute fixes, remove any duplicated stations
dupes <- filter(RegionalResults5, duplicated(FDT_STA_ID))
#RegionalResults6 <- filter(RegionalResults5, FDT_STA_ID %in% dupes$FDT_STA_ID)

# fucking same duplicates

dupes1 <- filter(template, duplicated(FDT_STA_ID))
template1 <- filter(template, FDT_STA_ID %in% dupes1$FDT_STA_ID)
write.csv(template1,'help.csv',row.names = F)


#Split into lake or riverine
RegionalResults5 $RorL <- NA
for(i in 1: nrow(RegionalResults5)){
  RegionalResults5 $RorL[i] <- substr(strsplit(as.character(RegionalResults5 $ID305B_1[i]), '-')[[1]][2] ,4,4)
  
}

# Make the rest riverine
lakeStations <- filter(RegionalResults5, RorL == 'L') %>%
  select(-RorL)

riverStations <- filter(RegionalResults5, RorL == 'R') %>%
  select(-c(RorL))

# Make sure we have everyone
nrow(lakeStations) + nrow(riverStations) == nrow(RegionalResults5)


regionCode <- 'BRRO'
write.csv(select(riverStations,-rowname), paste('processedStationData/final2020data/RegionalResultsRiverine_',
                                  regionCode,'CitMonNonAgencyFINAL.csv',sep=''), row.names=FALSE)

write.csv(select(lakeStations,-rowname), paste('processedStationData/final2020data/RegionalResultsLake_',
                                  regionCode,'CitMonNonAgencyFINAL.csv',sep=''), row.names=FALSE)
saveRDS(RegionalResults5,'processedStationData/final2020data/RegionalResults5.RDS')


```




Before we can run the riverine app, we need to attach correct Ecoregion information. Benthics data needs ecoregion attached to unique sites to determine SCI to use.



```{r ecogregion}

# Bring in Level 3 ecoregion
library(sf)
ecoregion <- st_read('GIS/VA_level3ecoregion.shp') %>%
  st_transform( st_crs(4326)) # transform to WQS84 for spatial intersection

# Bring back in assessment regions information one last time because conventionals
#doesn't have Basin in 2020

assessmentLayer <- st_read('GIS/AssessmentRegions_VA84_basins.shp') %>%
  st_transform( st_crs(4326)) # transform to WQS84 for spatial intersection


# Make conventionals spatial object
conventionals_sf <- conventionals3 %>%
  distinct(FDT_STA_ID, .keep_all = T) %>%
  st_as_sf( coords = c("Longitude", "Latitude"), 
            remove = F, # don't remove these lat/lon cols from df
            crs = 4326)  # add projection, needs to be geographic bc entering lat/lng

# make sure everything will work
identical(st_crs(conventionals_sf),st_crs(ecoregion))
identical(st_crs(conventionals_sf),st_crs(assessmentLayer))

# first get Basin information for conventionals, then Ecoregion
conventionals_sf <-  st_join(conventionals_sf, 
                             select(assessmentLayer, Basin), join = st_intersects) %>%
  st_join(select(ecoregion, US_L3CODE, US_L3NAME), join = st_intersects) %>%
  st_set_geometry(NULL) # take away spatial component so it can go into csv nicely

saveRDS(conventionals_sf, 'data/conventionals_sf_final2020_citmonNonAgency.RDS')

```
























Contact Emma Jones (emma.jones@deq.virginia.gov) if you have any questions about this script.
