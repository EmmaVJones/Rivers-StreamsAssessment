---
title: "Preparing Data for Regions"
output: 
  html_document:
    theme: yeti
---

Run in R 3.5.2

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(readxl))
suppressPackageStartupMessages(library(sf))
suppressPackageStartupMessages(library(shiny))
suppressPackageStartupMessages(library(mapview))
suppressPackageStartupMessages(library(leaflet))
suppressPackageStartupMessages(library(miniUI))
suppressPackageStartupMessages(library(DT))
suppressPackageStartupMessages(library(RColorBrewer))

source('snappingFunctions/snapFunctions.R') # snapping functions
source('snappingFunctions/snapOrganizationFunctions_citmon.R') # functions to do stuff with snapping functions
```

This document walks users through the requisite data preprocessing steps to prepare CitMon data for the 2020 IR Rivers and Streams Assessment and 2020 IR Lakes Assessment decision support applications. All initial steps are tested on 2016 and 2018 IR data and thus need to be rerun for 2020 data when those datasets become available.

This document demonstrates the necessary steps to get any regional office up to speed with prerequisite data organization/processing steps to run the 2020 IR. 

## Input data

#### CitMon
Bring in James' citizen monitoring dataset. I made it a csv before bringing it into RStudio to speed up rendering in app. 


```{r citmon2020, echo=F}
# too big to read in using read_excel
cit <- read_csv('C:/HardDriveBackup/R/GitHub/Rivers-StreamsAssessment/CitizenNonAgency/2020IR Citizen Ambient4.14.19 (2).csv') %>%
  dplyr::select(Group_Station_ID:`DEQ QA Comments`) # drop junk columns

#View(cit) # to look at dataset use this command
```

Bwork with spatial aspect, play with different ways of getting unique identifiers- group station Id (citizen assigned) and lat/long combos

```{r}

# Count occurrences of each Group_Station_ID
#cit1 <- dplyr::select(cit, Group_Station_ID, FDT_STA_ID, STA_DESC, Latitude, Longitude)

cit1 <- distinct(cit, Group_Station_ID, FDT_STA_ID, Latitude, Longitude, .keep_all =T)  %>%
  dplyr::select(Group_Station_ID,  FDT_STA_ID:FDT_SPG_CODE, Latitude:STA_CBP_NAME)# drop data to avoid any confusion

citCounts <- cit1 %>% 
  group_by(Group_Station_ID, Latitude, Longitude) %>%
  summarize(`Number of Station Sampling Events` = n())

citDetailscit <- cit1 %>% 
  group_by(Group_Station_ID, Latitude, Longitude) %>%
  mutate(`Number of Station Sampling Events` = n(),
         STA_DESC_norm = str_replace_all(STA_DESC, "[^[:alnum:]]", " ")) %>%
  ungroup() %>%
  distinct() %>%
  arrange(Group_Station_ID) 
citDetailscit$rowname <- c(1:nrow(citDetailscit))
# ugly addition singe add_rownames() alter data type


missingLocation <- filter(citDetailscit, is.na(Latitude) | is.na(Longitude) | Latitude == 0 | Longitude == 0)

citDetailscit_sf <- citDetailscit %>%
  filter(!(rowname %in% missingLocation$rowname)) %>%
  st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
           remove = F, # don't remove these lat/lon cols from df
           crs = 4326) # add coordinate reference system, needs to be geographic for now bc entering lat/lng,

```



Below is the table output. For this cycle, there are `r nrow(citDetailscit_sf)` unique stations sampled.

```{r conventionalsDistinctTable, echo=FALSE}
#View(citDetailscit_sf)
```


#### Statewide Assessment Layer

Because we cannot trust the assigned region code columns to give us the stations each region needs to assess, we must do a spatial subset of all unique stations in an IR window (citDetailscit_sf) by the Statewide Assessment Layer (filtered to our specified assessment region).

First, read in the statewide assessment layer.

```{r Statewide Assessment Layer, echo=FALSE, message=FALSE, warning=FALSE, results = 'hide'}
assessmentLayer <- st_read('GIS/AssessmentRegions_VA84_basins.shp') %>%
  st_transform( st_crs(4326)) # transform to WQS84 for spatial intersection
```

Now we are going to subset all unique stations from conventionals to a specific region. **This requires minimal user input**
 
Below are the regional codes you need to choose from 
```{r regionalCodes}
unique(assessmentLayer$ASSESS_REG)
```

Insert one of those unique codes in quotes into the chunk below. The commented out script serves as an example for you.

```{r establish regionCode}

regionCode <- ''
#regionCode <- 'BRRO'
```


Now we subset the layer based on your regionCode variable. We will use this variable again later in script.

```{r DEQ region selection}
assessmentLayerSelection <- filter(assessmentLayer, ASSESS_REG == regionCode)
#assessmentLayerSelection <- filter(assessmentLayer, ASSESS_REG ==    )
#assessmentLayerSelection <- filter(assessmentLayer, ASSESS_REG == 'BRRO')
```

Now we can subset unique citmon sites that fall into the assessment region we want.

```{r regional citmon, echo=F}

citmon_D_Region <- st_join(citDetailscit_sf, assessmentLayerSelection, join = st_intersects) %>%
  filter(!is.na(VAHU6)) %>% # only keep rows that joined
  dplyr::select(rowname, everything()) %>%
  select(-c(Shape_Leng, Shape_Area))
 
cat(paste('Below are the ',nrow(citmon_D_Region),' unique stations in the ', regionCode,' region.', sep=''))
```

```{r DistinctSitesInRegionTable, echo=FALSE}
#View(citmon_D_Region)
```


#### Last Cycle Estuary Assessment Layer

First thing we want to do now is filter out any sites that fall into the estuary AUs. These stations will have to be assessed through other means. **Note: you can download the table below for reference by running the commented script. These are all the stations within the selected assessment region that fall into an estuary AU based on the 2018 AU layers.**

The below chunk:
1) Reads in the 2018 draft estuary AU layer
2) Finds any sites that fall into an estuary AU polygon
  - see optional code to write those sites to a separate csv for future reference
3) Removes any estuarine sites from the data frame of unique conventionals sites that need AU and WQS information.

```{r Filter Out Estuary Statewide, echo=F, results = 'hide'}
estuaryAUs <- st_read('GIS/draft2018_spatialData/va_2018_aus_estuarine.shp') %>%
   st_transform( st_crs(4326)) # transform to WQS84 for spatial intersection


citmon_D_Region_Estuary <- st_join(citmon_D_Region, estuaryAUs, join = st_intersects) %>%
      filter(!is.na(OBJECTID))

# Remove the # if you want to run any of the lines below to save out the estuary sites to look at now. You will save them out at the end of the script, so this is optional right now.

#write.csv(conventionals_D_Region_Estuary, 'conventionals_D_Region_Estuary.csv', row.names = F) 
# you can change the file location like so:
#write.csv(conventionals_D_Region_Estuary, 'C:/Assessment/2020/dataProcessing/conventionals_D_Region_Estuary.csv', row.names = F) 

citmon_D_Region_NoEstuary <- filter(citmon_D_Region, !(FDT_STA_ID %in% citmon_D_Region_Estuary$FDT_STA_ID))
rm(estuaryAUs) # remove from memory
```

Below is the table of sites that are estuarine.

```{r estuarySitesTable, echo=F}
#View(citmon_D_Region_Estuary)
```

```{r inlineText2,echo=F}
cat(paste('Removing the above sites, the unique citmon sites have gone from ', nrow(citmon_D_Region),' to ',nrow(citmon_D_Region_NoEstuary),'. The next step is to find the lake/reservoir stations and link them to the appropriate AU and WQS for analyses in the Lake Assessment App.', sep=''))
```

### Assessment Unit Organization

#### Last Cycle Stations Table

The next step uses the 2018 IR station table as a starting point for linking the stations from the 2020 IR window to the appropriate AU. This is important to do prior to doing any spatial subsetting or snapping because most of the stations from the previous cycle already have QAed assessment unit information readily available and organized. It is computationally less expensive to save any necessary spatial subsetting or snapping to lake/reservoir polygons or stream segments until all joins that can be done with table methods are first completed.

```{r previousCycleStationTable}
# Statewide (mostly final) stations table from Cleo ( April 19, 2019), I exported the table to excel to avoid changing to 32bit version of R. DEQ only has 32 bit version of MS Office products, so connecting to Access databases requires changing the R program slightly, which is a pain in the middle of the script and going from 64 bit to 32 bit version of R just to connect to Access unnecessarily slows other processing times, so I made it easier with the Excel export step.

previousCycleStationTable <- read_excel('data/final2020data/tbl_ir_mon_stations.xlsx') %>%
  filter(REGION == regionCode)


# Join unique sites from current cycle to last cycle's station table to see what already has AU info
citmon_D_Region_AU <- mutate(citmon_D_Region_NoEstuary, STATION_ID = FDT_STA_ID) %>% # make joining column
    left_join(previousCycleStationTable, by='STATION_ID') # join to get ID305B info

# Only work with sites that don't already have AU info
citmon_D_Region_NoAU <- filter(citmon_D_Region_AU, is.na(ID305B_1))
#View(conventionals_D_Region_NoAU)  

# Keep the stations with AU information to the side for later use
citmon_D_Region_AU <- filter(citmon_D_Region_AU, !is.na(ID305B_1))
#View(conventionals_D_Region_AU)

# make sure we didnt lose anyone
nrow(citmon_D_Region_NoAU) + nrow(citmon_D_Region_AU) == nrow(citmon_D_Region) 

rm(citmon_D_Region_NoEstuary) # clean up workspace
```


The table below illustrates all the  unique citmon sites that matched stations from 2018 IR station table by STATION_ID/FDT_STA_ID.
```{r citmon_D_Region_AU, echo=F}
#View(citmon_D_Region_Estuary)
```

The table below illustrates all the unique citmon sites that did not match any stations from 2018 IR station table by STATION_ID/FDT_STA_ID.
```{r citmon_D_Region_NoAU, echo=F}
#View(citmon_D_Region_Estuary)
```

#### Last Cycle Reservoir Assessment Layer 

```{r inlineText3, echo=F}
cat(paste('Working with the ', nrow(citmon_D_Region_NoAU),' sites that did not connect to a STATION_ID/FDT_STA_ID from last cycle, we will first do a spatial join to the reservoir/lakes AUs to see if any sites are lake sites before using the more computationally intensive stream snapping process.', sep=''))
```

```{r reservoirStatewide, echo=F, results = 'hide'}
reservoirAUs <- st_read('GIS/draft2018_spatialData/va_2018_aus_reservoir.shp') %>%
   st_transform( st_crs(4326)) # transform to WQS84 for spatial intersection

# find lake AUs, if Any
citmon_D_Region_Lakes <- st_join(citmon_D_Region_NoAU, reservoirAUs, join = st_intersects) %>%
  filter(!is.na(OBJECTID)) %>% # only keep rows where join successful
  mutate(ID305B_1 = ID305B) %>% # rename ID305B_1 field by the joined reservoir ID305B
  dplyr::select(names(citmon_D_Region_AU))

# add lake AU sites to citmon_D_Region_AU
citmon_D_Region_AU <- rbind(citmon_D_Region_AU, citmon_D_Region_Lakes)

# and remove lake AU sites from the sites without AUs
citmon_D_Region_NoAU <- filter(citmon_D_Region_NoAU, !(rowname %in% citmon_D_Region_Lakes$rowname))#filter(citmon_D_Region_NoAU, !(FDT_STA_ID %in% citmon_D_Region_Lakes$FDT_STA_ID))

# make sure we didn't lose anyone
nrow(citmon_D_Region_NoAU) + nrow(citmon_D_Region_AU) == nrow(citmon_D_Region) 

rm(reservoirAUs); rm(citmon_D_Region_Lakes) # remove from memory
```

#### Last Cycle Riverine Assessment Layer

```{r inlineText4, echo=F}
cat(paste('The last step for AU organization after we have removed any estuary sites and dealt with all sites that are in lake AUs is to automatically snap the', nrow(citmon_D_Region_NoAU), 'remaining sites to riverine AUs.',sep=' '))
```

A custom, automated snapping tool is run below. The function buffers each site using the user defined sequence (bufferDistances). You do not need to change the bufferDistances variable, but commented out code demonstrates how you can if you want to increase/decrease the maximum buffer distance or breaks. 

The function first buffers the site the lowest input buffer distance, then it intersects that buffer with the input assessment unit layer. If no segments are retrieved in the given buffer distance, then the next highest buffer is tried and so on until the maximum buffer distance is tested. If one or more segments are captured in a given buffer, then the buffering process stops, returns the snapped stream segment information, and moves to the next site. If more that one site is captured in a single buffer, then those segments will be provided to the user to choose from in an interactive gadget to keep only one AU per site after all buffering has finished. If the maximum buffer distance does not yield any stream segments, then those sites will be returned to the user for special QA and manual table completion after the report is finished. It is advised that you investigate the site further and potentially correct site coordinates in CEDS to prevent future problems. 

**Note: All stations you want to use in the assessment applications require AU information. If no AU information is provided to the app with a StationID, then the station will not appear in the app and could potentially be lost until further QA.**

The next chunk will bring in the riverine AU layer from the previous cycle, complete the snapping tool, ask for user input in the form of a gadget in the viewer panel if sites snap to multiple segments in a single buffer, and then organize results. You can see the progress and results of the tool below the chunk.

```{r riverineAUsnap, echo=FALSE}
# use a custom snapping tool to snap and organize 
riverineAUs <-  st_read('GIS/draft2018_spatialData/va_2018_aus_riverine.shp') %>%
  st_transform(crs = 102003)  # transform to Albers for spatial intersection

Regional_Sites_AU <- snapAndOrganizeAU_citmon(citmon_D_Region_NoAU, riverineAUs,  bufferDistances = seq(10,80,by=10))
# this buffers up to 100 m by 10 m
#Regional_Sites_AU <- snapAndOrganizeAU(citmon_D_Region_NoAU, riverineAUs, bufferDistances = seq(10,100,by=10)) 
# this buffers up to 50 m by 5 m
#Regional_Sites_AU <- snapAndOrganizeAU(citmon_D_Region_NoAU, riverineAUs, bufferDistances = seq(10,50,by=5))

rm(riverineAUs) #clean up workspace

```

We can now filter the results to figure out sites that worked and sites that still need special attention. 

Below are the sites that still need work.
```{r conventionals_D_Region_NoAU still, echo=F}
citmon_D_Region_NoAU <- filter(Regional_Sites_AU, is.na(ID305B_1)) %>%
  left_join(dplyr::select(citmon_D_Region, rowname:FDT_STA_ID, Latitude, Longitude) %>% st_drop_geometry(), 
            by = c( 'FDT_STA_ID', 'Latitude', 'Longitude')) %>% # join back to original dataset that went into snapping function to get rowname and Group_Station_ID
  dplyr::select(rowname, Group_Station_ID, everything())

#View(conventionals_D_Region_NoAU)
```

These sites will still be snapped to WQS, but they still need AU information before they can be run through the assessment applications. A reminder at the end of the report will encourage users to complete that step.


Below are the sites that worked. These can be combined with our conventionals_D_Region_AU layer now. 
```{r Regional_Sites_AUgood, echo=F}
Regional_Sites_AU <- filter(Regional_Sites_AU, !is.na(ID305B_1)) %>%
  left_join(dplyr::select(citmon_D_Region, rowname:FDT_STA_ID, Latitude, Longitude) %>% st_drop_geometry(), 
            by = c( 'FDT_STA_ID', 'Latitude', 'Longitude')) %>% # join back to original dataset that went into snapping function to get rowname and Group_Station_ID
  dplyr::select(rowname, Group_Station_ID, everything())

#View(Regional_Sites_AU)

# only bind citmon_D_Region_NoAU if it has records, otherwise the bind won't work
if(nrow(Regional_Sites_AU) > 0 & nrow(citmon_D_Region_NoAU) > 0 ){
  citmon_D_Region_AU <- rbind(citmon_D_Region_AU, 
                              Regional_Sites_AU %>% st_transform( st_crs(4326)), 
                              citmon_D_Region_NoAU %>% st_transform( st_crs(4326)))}
if(nrow(Regional_Sites_AU) > 0 & nrow(citmon_D_Region_NoAU) == 0 ){
  citmon_D_Region_AU <- rbind(citmon_D_Region_AU, 
                              Regional_Sites_AU %>% st_transform( st_crs(4326)))}
if(nrow(Regional_Sites_AU) == 0 & nrow(citmon_D_Region_NoAU) > 0 ){
  citmon_D_Region_AU <- rbind(citmon_D_Region_AU, 
                               citmon_D_Region_NoAU %>% st_transform( st_crs(4326)))}

# save just AU info
write.csv(citmon_D_Region_AU, 'processedStationData/citmon/BRRO_AU_citmon.csv', row.names = F)
st_write(citmon_D_Region_AU, 'processedStationData/citmon/BRRO_AU_citmon.shp')

rm(Regional_Sites_AU) # clean up workspace
```

















### Assessment QA time out

So those above datasets need to be thoroughly reviewed and QAed to make sure each station is in fact unique and have all the correct associated data with it. When that is done, you can bring the data back in.

```{r citmon timeIn}
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(readxl))
suppressPackageStartupMessages(library(sf))
suppressPackageStartupMessages(library(shiny))
suppressPackageStartupMessages(library(mapview))
suppressPackageStartupMessages(library(leaflet))
suppressPackageStartupMessages(library(miniUI))
suppressPackageStartupMessages(library(DT))
suppressPackageStartupMessages(library(RColorBrewer))

source('snappingFunctions/snapFunctions.R') # snapping functions
source('snappingFunctions/snapOrganizationFunctions_citmon.R') # functions to do stuff with snapping functions

citMonAU <- read_excel('processedStationData/citmon/CitMon_stations_QA_Final.xlsx','Paula_NEW')

monitor <- read_excel('processedStationData/citmon/MONITOR.xlsx')
```
 
First let's delete any weird things they want gone and then it is time to correct the FDT_STA_ID based on Mary and Paula's calls. We will replace the FDT_STA_ID assigned by James with Mary and Paula's call and then update all the associated information accordingly based on the MONITOR database.

```{r delete weird things}
# get the rows they want deleted
citMonAU1 <- filter(citMonAU, !(grepl('delete',citMonAU$`AllMon station`)))
```

```{r split based on Mary/paula overrule}
fine <- filter(citMonAU1, is.na(`AllMon station`)) %>%
  dplyr::select(-c(`AllMon station`))
notfine <- filter(citMonAU1, !is.na(`AllMon station`))
```

Start fixing the not fine.

```{r fixNotFine}
notfine1 <- rename(notfine, 'STATION' = 'AllMon station') %>%
  left_join(monitor, by='STATION') 

# fix broken sites now

notfine2 <- filter(notfine1, !is.na(TRECORD)) %>% # only keep things that worked
  mutate(FDT_STA_ID = STATION, STA_DESC = DESCRIP, Latitude = MDEC_LAT, Longitude = MDECLONG) %>%
  dplyr::select(`AU to connect to`,names(fine))


fineTogether <- rbind(fine, notfine2) %>%
  dplyr::select(-c(Group_Station_ID,STATION_ID:`...102`))

rm(fine);rm(notfine);rm(notfine1);rm(notfine2)
```

Now join back to station table from last cycle to get better AU information since mary/paula changed some AUs.

```{r fix more station info}
regionCode <- 'BRRO'

previousCycleStationTable <- read_excel('data/final2020data/tbl_ir_mon_stations.xlsx') %>%
  filter(REGION == regionCode)

fineTogether1 <- mutate(fineTogether, STATION_ID= FDT_STA_ID) %>%
  left_join(previousCycleStationTable, by= 'STATION_ID') %>%
  select(`AU to connect to`, ID305B_1, everything())

questions <- c('4ABDA-T16-FC', '4AIND-T20-FC', '4AGIL-T0-FC', '4AGIL-T2-FC', '4APCP-T4-FC', '4ABSA-T6-FC', '4AXNX-T8-FC', '4AXNY-T7-FC', '4ACCK-T12-FC', '4ASWC-T13-FC', '4AJUM-T15-FC')

z <- filter(fineTogether1, FDT_STA_ID %in% questions)
View(z)
write.csv(z, 'processedStationData/citmon/changedAUs.csv',row.names=F)

  mutate(ID305B_1= ifelse(is.na(ID305B_1),`AU to connect to`,ID305B_1)) %>%
  dplyr::select(-`AU to connect to`)


cit <- fineTogether1 %>%
  st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
           remove = F, # don't remove these lat/lon cols from df
           crs = 4326) # add coordinate reference system, needs to be geographic for now bc entering lat/lng,


rm(monitor);rm(previousCycleStationTable); rm(citMonAU); rm(fineTogether)
```


So now we have a dataset that has corrected station ID's and AU's. Note that there are still duplicated entries, but that is good until I limit out the raw data by the rowname field.


Now time to do WQS things.




### WQS Organization

Now all the stations need water quality standards attached as the final data organization step in order to be used by the assessment apps. Regardless of whether or not a site snapped to an AU, the report will walk users through connecting each site to WQS features. First, we will start with lakes to save the computationally heavier stream snapping tool for only the sites that need it.

#### Updated Reservoir WQS layer

We will bring in the updated WQS reservoir layer, to attach to stations that are in lacustrine AU's. There are three methods below for identifying your lacustrine stations to choose from. Only one needs to be run to establish a reservoirStations object to join to reservoir WQS information. 

```{r find Lacustrine AUs, echo=F}
# Bring reservoir AUs back in just for ID305B options
#reservoirAUs <- as.character(st_read('GIS/draft2018_spatialData/va_2018_aus_reservoir.shp')$ID305B)

# Method 1: use lake AU's
# Separate lake stations from stream stations
# this is one way, but since we have Paula's official station table, we will use that instead
#reservoirStations <- filter(conventionals_D_Region_AU, ID305B_1 %in% unique(reservoirAUs) |
#                              ID305B_2 %in% unique(reservoirAUs) | ID305B_3 %in% unique(reservoirAUs))

# Method 2: Look for 'L' designation in station type codes
#reservoirStations <- filter(conventionals_D_Region_AU,  grepl("L",STATION_TYPE_1) | grepl("L",STATION_TYPE_2) |
#                               grepl("L",STATION_TYPE_3))
# the L searching method is safer because it captures the ,L situations where L tacked on to other station designation

# Method 3: use original lake stations table
# but the safest method for BRRO is to just use Paula's stations table
#reservoirStations <- filter(conventionals_D_Region_AU, FDT_STA_ID %in% stationTableLake$STATION_ID)



# Use Assessor defined AU's to figure out if lake or stream

cit$RorL <- NA
for(i in 1: nrow(cit)){
  cit$RorL[i] <- substr(strsplit(as.character(cit$ID305B_1[i]), '-')[[1]][2] ,4,4)
  
}


# split into riverine or lake
reservoirStations <- filter(cit, RorL == 'L')

# Bring in reservoir WQS
reservoirWQS <- st_read('GIS/updatedWQS/reservoir.shp') %>%
   st_transform( st_crs(4326)) # transform to WQS84 for spatial intersection

# find lake WQS
reservoirStationsWQS <- st_join(reservoirStations, reservoirWQS, join = st_intersects) %>%
  filter(!is.na(OBJECTID)) # only keep rows where join successful
#View(reservoirStationsWQS)

# add back in duplicate issues where lacustrine and riverine, important where stations used for both riverine and lacustrine assessments
notInReservoirWQS <- unique(reservoirStations$FDT_STA_ID)[!( unique(reservoirStations$FDT_STA_ID) %in% unique(reservoirStationsWQS$FDT_STA_ID))]




problem <- filter(reservoirStations, FDT_STA_ID %in% notInReservoirWQS)
st_write(problem,'processedStationData/citmon/lakeStationsNotInLakeWQS.shp')





# Run all stations that didn't connect to lake WQS polygon through streams snapping tool
riverineStationsWQS <- filter(cit, !(FDT_STA_ID %in% reservoirStationsWQS$FDT_STA_ID) |
                                FDT_STA_ID %in% notInReservoirWQS)
                              
# Make sure we didn't lose anyone
nrow(riverineStationsWQS) + nrow(reservoirStationsWQS) == nrow(cit) 

rm(reservoirWQS);rm(reservoirAUs); rm(reservoirStations) # clean up workspace
```


All stations that did not attach to a lake WQS will be run below as a stream site. This stream WQS snapping tool behaves the same way as the AU snapping tool; however, we are bringing in basins for snapping one at a time because the WQS files are so large. The tool will handle the organization and snapping by basin. You are only responsible for using the gadget that pops up in the Viewer panel when each basin prompts you. You can see the progress below the chunk. This is the most computationally heavy part of the process.


```{r attachStreamWQS, echo=F}
regionalOutput <- list()
# Loop through multiple basins
for (i in 1:length(unique(riverineStationsWQS$Basin))){
  Regional_Sites_AU_basin <- filter(riverineStationsWQS, Basin %in% unique(riverineStationsWQS$Basin)[i] )
  WQSsnaps <- snapAndOrganizeWQS_citmon(Regional_Sites_AU_basin, 'GIS/updatedWQS/', 
                                 basinNameSwitcher(unique(Regional_Sites_AU_basin$Basin)),  seq(10,80,by=10))
  regionalOutput[[i]] <- WQSsnaps
}
```

Now the list gets smashed together into a single dataframe (tibble).

```{r smash}

regionalOutput[[1]] <- mutate(regionalOutput[[1]], FDT_STA_ID = FDT_STA_ID.x) %>% select( -c( FDT_STA_ID.x,FDT_STA_ID.y)) %>% select(rowname, FDT_STA_ID, everything())

# smash it all back together, change from lists to single dataframe
RegionalResults <- do.call("rbind", lapply(regionalOutput, data.frame))
#View(RegionalResults)
rm(WQSsnaps)# clean up workspace
```

Below is the table of riverine stations without WQS information attached. These will be returned to the user with a full list of stations that need individual attention. 

```{r RegionalResults table, echo=F}
RegionalResultsNoWQS <- filter(RegionalResults, is.na(OBJECTID))
#View(RegionalResultsNoWQS)
```

Lastly, we need to combine the lake and stream stations into a single dataframe. After we combine we will separate out the stations that need either AU or WQS manual entry. We suggest using GIS to find this information. 

```{r combine WQS, echo=F}
# Reorder columns in reservoirStationsWQS to match RegionalResults to allow row binding
reservoirStationsWQS <- mutate(reservoirStationsWQS, Point.Unique.Identifier=NA, 
                               Buffer.Distance=NA, StreamType=NA) %>%
  select(FDT_STA_ID:COMMENTS, Point.Unique.Identifier, Buffer.Distance, 
         OBJECTID:FCode, BASIN, WQS_COMMEN, WATER_NAME, SEC, CLASS, 
         SPSTDS, SECTION_DE, Basin_Code, PWS, Trout, Edit_Date, StreamType, 
         Tier_III, Backlog, Shape_Leng)%>%
  st_set_geometry(NULL)

RegionalResultsCombined <- rbind(RegionalResults,reservoirStationsWQS)
#View(RegionalResultsCombined)
```


This next chunk walks users through saving all the results. The results are saved to a default folder created in this directory called processedStationData. You can find the datasets you need to manually update before entering into the app there. Again, AU and WQS information as formatted by the above code is required for all sites you wish to analyze within the assessment apps. Failure to upload stations in the correct format ensures the app with either break or fail to analyze your data. It is advised that you fix the stations in the RegionalResultsCombined_REGIONCODE.csv. Other spreadsheets and shapefiles of stations missing information are provided for you to use in GIS to make corrections in the RegionalResultsComnined.csv.

```{r saveResults, echo=FALSE}
# This table has all unique stations within the selected region, regardless of AU or WQS connections.
write.csv(RegionalResultsCombined, paste('processedStationData/final2020data/RegionalResultsCombined_',
                                         regionCode,'.csv',sep=''), row.names=FALSE)

# This table and shapefile highlights the stations that need AU information prior to use in assessment apps
RegionalResultsCombined_NoAU <- filter(RegionalResultsCombined, is.na(ID305B_1))
write.csv(RegionalResultsCombined_NoAU,
          paste('processedStationData/final2020data/RegionalResultsCombined_NoAU_',      
                regionCode,'.csv',sep=''), row.names=FALSE)
#shapefile version of same data
RegionalResultsCombined_NoAU_sf <- RegionalResultsCombined_NoAU %>%
  st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
           remove = F, # don't remove these lat/lon cols from df
           crs = 4326) # add coordinate reference system, needs to be geographic for now bc entering lat/lng,
st_write(RegionalResultsCombined_NoAU_sf,
         paste('processedStationData/final2020data/RegionalResultsCombined_NoAU_',
               regionCode,'.shp',sep=''))

# This table and shapefile highlights the stations that need WQS information prior to use in assessment apps
RegionalResultsCombined_NoWQS <- filter(RegionalResultsCombined, is.na(OBJECTID))
write.csv(RegionalResultsCombined_NoWQS,
          paste('processedStationData/final2020data/RegionalResultsCombined_NoWQS_',
                regionCode,'.csv',sep=''), row.names=FALSE)
#shapefile version of same data
RegionalResultsCombined_NoWQS_sf <- RegionalResultsCombined_NoWQS %>%
  st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
           remove = F, # don't remove these lat/lon cols from df
           crs = 4326) # add coordinate reference system, needs to be geographic for now bc entering lat/lng,
st_write(RegionalResultsCombined_NoWQS_sf,
         paste('processedStationData/final2020data/RegionalResultsCombined_NoWQS_',
               regionCode,'.shp',sep=''))


# This table has all Estuary sites for your reference. None of the available assessment apps currently handle estuarine assessments
write.csv(conventionals_D_Region_Estuary,
          paste('processedStationData/final2020data/EstuarineSites_',
                regionCode,'.csv',sep=''), row.names=FALSE)
```


## Split out riverine vs lake stations for assessment apps

Once you fix any stations that are missing AU or WQS information, you can read the entire dataset back to this script and have the program split the station tables into stations that will be analyzed by the lake assessment app and those that will be assessed by the rivers and streams assessment app. **All stations that need to be assessed by the lake assessment app require a 'L' in either STATION_TYPE_1, STATION_TYPE_2, or STATION_TYPE_3 fields.** Packages and necessary datasets are reloaded in case you are not completing this step in the same session as the previous AU/WQS assignment session. 

**Minimal user input: if the file or file location you want to upload differs from the example given, you will need to update it to work with your configuration.**

If you are starting from a new session in R, you need to remind it what region you are working with. Use the same region code as specified above.

```{r remindR}
regionCode <- ''
#regionCode <- 'BRRO'
```


```{r splitRiversAndLakes, echo=F}
library(tidyverse)

# This is the station table you have edited including all riverine and lake stations with all necessary AU and WQS information fixed for your region.
#RegionalResultsCombinedAll <- read_csv('processedStationData/final2020data/RegionalResultsCombined_final.csv')
RegionalResultsCombinedAll <-read_csv(' XXXX .csv')

# Quick duplicate check
dupes <- RegionalResultsCombinedAll$FDT_STA_ID [duplicated(RegionalResultsCombinedAll$FDT_STA_ID )] 


# Find all stations that have lake designations
RegionalResultsCombinedAll3 <- mutate(RegionalResultsCombinedAll2, 
                                      lakeStation = ifelse(grepl("L",STATION_TYPE_1) |
                                                             grepl("L",STATION_TYPE_2) |
                                                             grepl("L",STATION_TYPE_3), 'lake','river'))

lakeStations <- filter(RegionalResultsCombinedAll3, lakeStation=='lake') %>%
  select(-lakeStation)

# Make the rest riverine
riverStations <- filter(RegionalResultsCombinedAll3, lakeStation != 'lake') %>%
  select(-lakeStation)

# Make sure we didnt lose anyone
nrow(riverStations) + nrow(lakeStations) == nrow(RegionalResultsCombinedAll3)

# Save both for app use
write.csv(lakeStations, paste('processedStationData/final2020data/RegionalResultsLake_',
                                         assessmentLayerSelection,'FINAL.csv',sep=''), row.names=FALSE)
write.csv(riverStations, paste('processedStationData/final2020data/RegionalResultsRiverine_',
                                         assessmentLayerSelection,'FINAL.csv',sep=''), row.names=FALSE)


```

Before we can run the riverine app, we need to attach correct Ecoregion information. Benthics data needs ecoregion attached to unique sites to determine SCI to use.

```{r benthics ecoregion join}

# Bring back in original Conventionals data
conventionals <- read_csv('data/final2020data/CEDSWQM_2020_IR_DATA-CONVENTIONALS_20190305.csv') %>%
  filter(!is.na(Latitude)|!is.na(Longitude)) %>% # remove sites without coordinates
 # rename changed column names since app was built
  rename("FDT_TEMP_CELCIUS"  ="TEMPERATURE_00010_DEGREES CENTIGRADE",
         "FDT_TEMP_CELCIUS_RMK" = "FDT_TEMP_CELCIUS_RMK",  
         "FDT_FIELD_PH" = "pH_00400_STD_UNITS" ,          
         "FDT_FIELD_PH_RMK"  ="FDT_FIELD_PH_RMK", 
         "DO" =  "DO_mg/L",       
         "DO_RMK"  ="DO_RMK",    
         "FDT_SPECIFIC_CONDUCTANCE"="SPECIFIC_CONDUCTANCE_00095_UMHOS/CM @ 25C",    
         "FDT_SPECIFIC_CONDUCTANCE_RMK" ="FDT_SPECIFIC_CONDUCTANCE_RMK" ,
         "FDT_SALINITY" = "SALINITY_00480_PPT" ,            
         "FDT_SALINITY_RMK"  ="FDT_SALINITY_RMK",  
         "NITROGEN" = "NITROGEN_mg/L" ,                    
         "AMMONIA" ="AMMONIA_mg/L",
         "PHOSPHORUS" =  "PHOSPHORUS_mg/L",
         "FECAL_COLI" = "FECAL_COLIFORM_31616_NO/100mL" ,
         "E.COLI" = "ECOLI_CFU/100mL",                       
         "ENTEROCOCCI" =  "ENTEROCOCCI_31649_NO/100mL",
         "CHLOROPHYLL" ="CHLOROPHYLL_32211_ug/L",                
         "SSC" ="SSC-TOTAL_00530_mg/L" , 
         "NITRATE" ="NITRATE_mg/L", 
         "CHLORIDE" ="CHLORIDE_mg/L",
         "SULFATE_TOTAL" ="SULFATE_TOTAL_00945_mg/L",              
         "SULFATE_DISS" ="SULFATE_DISSOLVED_00946_mg/L")
conventionals$FDT_DATE_TIME2 <- as.POSIXct(conventionals$FDT_DATE_TIME, format="%m/%d/%Y %H:%M")


# Bring in Level 3 ecoregion
library(sf)
ecoregion <- st_read('GIS/VA_level3ecoregion.shp') %>%
  st_transform( st_crs(4326)) # transform to WQS84 for spatial intersection

# Bring back in assessment regions information one last time because conventionals doesn't have Basin in 2020
assessmentLayer <- st_read('GIS/AssessmentRegions_VA84_basins.shp') %>%
  st_transform( st_crs(4326)) # transform to WQS84 for spatial intersection


# Make conventionals spatial object
conventionals_sf <- conventionals %>%
  distinct(FDT_STA_ID, .keep_all = T) %>%
  st_as_sf( coords = c("Longitude", "Latitude"), 
            remove = F, # don't remove these lat/lon cols from df
            crs = 4326)  # add projection, needs to be geographic bc entering lat/lng

# make sure everything will work
identical(st_crs(conventionals_sf),st_crs(ecoregion))
identical(st_crs(conventionals_sf),st_crs(assessmentLayer))

# first get Basin information for conventionals, then Ecoregion
conventionals_sf <-  st_join(conventionals_sf, 
                          select(assessmentLayer, Basin), join = st_intersects) %>%
  st_join(select(ecoregion, US_L3CODE, US_L3NAME), join = st_intersects) %>%
  st_set_geometry(NULL) # take away spatial component so it can go into csv nicely

saveRDS(conventionals_sf, 'data/conventionals_sf_final2020.RDS')
```


Contact Emma Jones (emma.jones@deq.virginia.gov) if you have any questions about this script.
